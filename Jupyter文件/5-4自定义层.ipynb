{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.4 自定义层\n",
    "可以用创造性的方式组合不同的层，从而设计出适用于各种任务的架构。\n",
    "如用于处理图像、文本、序列数据和执行动态规划的层。 \n",
    "有时我们会遇到或要自己发明一个现在在深度学习框架中还不存在的层。 在这些情况下，必须构建自定义层。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 我们可以通过基本层类设计自定义层。这允许我们定义灵活的新层，其行为与深度学习框架中的任何现有层不同。\n",
    "\n",
    "+ 在自定义层定义完成后，我们就可以在任意环境和网络架构中调用该自定义层。\n",
    "\n",
    "+ 层可以有局部参数，这些参数可以通过内置函数创建。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4.1 不带参数的层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从输入当中减去均值的层\n",
    "import torch\n",
    "import torch.nn.functional as F \n",
    "from torch import nn\n",
    "\n",
    "class CenteredLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return X - X.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2., -1.,  0.,  1.,  2.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = CenteredLayer()\n",
    "layer(torch.FloatTensor([1,2,3,4,5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.9849e-09, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试浮点数 最后结果的mean应该为0 但是精度问题会导致最后是一个-inf\n",
    "# 将CenteredLayer组合到更大的网络当中\n",
    "net = nn.Sequential(nn.Linear(8,128), CenteredLayer())\n",
    "Y = net(torch.rand(2,8))\n",
    "Y.mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4.2 带参数的层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义版本的全连接层 W 和 bias\n",
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, in_units, units):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(in_units, units))\n",
    "        self.bias = nn.Parameter(torch.randn(units))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        linear = torch.matmul(X, self.weight.data) + self.bias.data\n",
    "        # 修正线性单元\n",
    "        return F.relu(linear)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.3275,  0.4442, -0.9269],\n",
      "        [ 0.1498,  0.1826, -0.1862],\n",
      "        [-2.1918,  2.5479, -1.0744],\n",
      "        [ 1.0381, -0.6894,  0.7876],\n",
      "        [-0.0142, -0.5855,  0.2298]], requires_grad=True)\n",
      "tensor([[ 1.3275,  0.4442, -0.9269],\n",
      "        [ 0.1498,  0.1826, -0.1862],\n",
      "        [-2.1918,  2.5479, -1.0744],\n",
      "        [ 1.0381, -0.6894,  0.7876],\n",
      "        [-0.0142, -0.5855,  0.2298]])\n"
     ]
    }
   ],
   "source": [
    "# 实例化MyLinear类并访问其参数\n",
    "linear = MyLinear(5,3)\n",
    "# torch.nn.parameter.Parameter类型\n",
    "print((linear.weight)) \n",
    "# tensor类型\n",
    "print((linear.weight.data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.3891, 0.0000],\n",
       "        [0.7395, 0.0000, 0.3609]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用自定义层直接执行前向传播计算\n",
    "linear(torch.rand(2,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3020],\n",
       "        [6.2328]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用自定义层构建模型，就像使用内置的全连接层一样使用自定义层\n",
    "net = nn.Sequential(MyLinear(64, 8), MyLinear(8, 1))\n",
    "net(torch.rand(2, 64))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y_k= {\\textstyle \\sum_{i}^{j}}W_{i,j,k}x_ix_j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomReductionLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(CustomReductionLayer, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.Tensor(in_channels, in_channels, out_channels))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Expand dimensions for element-wise multiplication\n",
    "        #  维度2扩展 （N,in_channels,1）\n",
    "        x_expanded = x.unsqueeze(2)\n",
    "        # 维度1扩展 (N,1,in_channels)\n",
    "        x_expanded_transpose = x.unsqueeze(1)\n",
    "        \n",
    "        # Element-wise multiplication and summation\n",
    "        y = torch.sum(x_expanded * x_expanded_transpose * self.weight, dim=(0, 1))\n",
    "        \n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand(N, in_channels)\n\u001b[0;32m     31\u001b[0m \u001b[39m# Compute the output tensor y using the reduction layer\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m y \u001b[39m=\u001b[39m reduction_layer(x)\n\u001b[0;32m     34\u001b[0m \u001b[39m# Print the input and output tensors\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mInput tensor x:\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mt:\\Programs\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[20], line 16\u001b[0m, in \u001b[0;36mCustomReductionLayer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     13\u001b[0m x_expanded_transpose \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39munsqueeze(\u001b[39m2\u001b[39m)  \u001b[39m# Shape: (N, in_channels, 1)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m# Element-wise multiplication\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m y_intermediate \u001b[39m=\u001b[39m x_expanded \u001b[39m*\u001b[39;49m x_expanded_transpose \u001b[39m*\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight  \u001b[39m# Shape: (N, in_channels, in_channels, out_channels)\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39m# Summation along appropriate dimensions\u001b[39;00m\n\u001b[0;32m     19\u001b[0m y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(y_intermediate, dim\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m))  \u001b[39m# Shape: (N, out_channels)\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 2"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
