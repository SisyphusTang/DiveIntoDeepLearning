{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "os.environ [\"KMP_DUPLICATE_LIB_OK\"] =\"TRUE\"\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当需要更强的灵活性时，我们需要定义自己的块。 例如，我们可能希望在前向传播函数中执行Python的控制流。 此外，我们可能希望执行任意的数学运算， 而不是简单地依赖预定义的神经网络层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ()当中为该类继承的父类\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() # 父类的init函数\n",
    "        self.hidden = nn.Linear(20,256)\n",
    "        self.out = nn.Linear(256,10)\n",
    "\n",
    "    def forward(self,X):\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5084, 0.0234, 0.2324, 0.8389, 0.2188, 0.0807, 0.2197, 0.9048, 0.5850,\n",
      "         0.8006, 0.8420, 0.6595, 0.2179, 0.1998, 0.5880, 0.7368, 0.9461, 0.3790,\n",
      "         0.8514, 0.0441],\n",
      "        [0.7092, 0.6483, 0.0623, 0.7688, 0.0254, 0.7000, 0.8813, 0.0878, 0.5291,\n",
      "         0.4179, 0.5552, 0.5370, 0.6381, 0.4795, 0.2424, 0.4252, 0.0023, 0.1759,\n",
      "         0.8189, 0.2468]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2372, -0.0454, -0.2730,  0.0371,  0.0904,  0.0771,  0.0566,  0.1171,\n",
       "         -0.1327, -0.1393],\n",
       "        [ 0.2900, -0.0497, -0.2357,  0.1661,  0.0385,  0.0370, -0.1158,  0.0748,\n",
       "         -0.1775, -0.0188]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "\n",
    "X = torch.rand(2, 20)\n",
    "net(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0955,  0.2500,  0.1953, -0.0202,  0.0477,  0.0377, -0.1918, -0.1803,\n",
       "          0.2415, -0.1874],\n",
       "        [-0.0999,  0.2789,  0.2507, -0.0682, -0.0976,  0.0323, -0.1187, -0.1226,\n",
       "          0.2054, -0.1948]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 与Sequential类相同的功能\n",
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "\n",
    "        for idx,moudle in enumerate(args):\n",
    "            # self._moudles是一个字典 \n",
    "            self._modules[str(idx)] = moudle\n",
    "\n",
    "    def forward(self,X):\n",
    "        # 依次计算每个moudle的输出 并作为下一个moudle的输出\n",
    "        for block in self._modules.values():\n",
    "            X = block(X)\n",
    "        \n",
    "        return X   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4050,  0.0859,  0.0217, -0.0808, -0.0752, -0.1994,  0.1723,  0.1233,\n",
       "         -0.1319,  0.0438],\n",
       "        [-0.1312,  0.0705,  0.0638, -0.1632, -0.0294, -0.2264,  0.0639,  0.0049,\n",
       "         -0.0778,  0.0396]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256,10))\n",
    "net(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rand_weight = torch.rand((20,20),requires_grad=False)\n",
    "        self.linear = nn.Linear(20,20)\n",
    "\n",
    "    \n",
    "    def forward(self,X):\n",
    "        X = self.linear(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
